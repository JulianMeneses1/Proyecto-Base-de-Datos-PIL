{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lectura de los datos contenidos en los archivos csv y generación de las tablas en mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tablas creadas con éxito\n"
     ]
    }
   ],
   "source": [
    "from Configuración_BD import engine\n",
    "import pandas as pd\n",
    "\n",
    "db = engine.connect()\n",
    "\n",
    "df_alumnos = pd.read_csv(\"Bases de datos/Alumnos.csv\",sep=\",\") # lee la base de datos \"alumnos\" que está en formato csv y lo guarda en la variable df\n",
    "df_profesores = pd.read_csv(\"Bases de datos/Profesores.csv\",sep=\",\") # lo mismo con \"profesores\"\n",
    "df_cursos_profesores = pd.read_csv(\"Bases de datos/Cursos_profesores.csv\",sep=\",\") # lo mismo con \"cursos_profesores\"\n",
    "\n",
    "\n",
    "try:\n",
    "\tdf_alumnos.to_sql(\"alumnos\", db, if_exists=\"fail\") # convierte el dataframe (df) de alumnos a sql y crea la tabla profes \n",
    "\tdf_profesores.to_sql(\"profesores\", db, if_exists=\"fail\") # lo mismo con \"profesores\"\n",
    "\tdf_cursos_profesores.to_sql(\"cursos_profesores\", db, if_exists=\"fail\") # lo mismo con \"cursos_profesores\" \n",
    "\n",
    "except ValueError as vx:\n",
    "\tprint(vx)\n",
    "except Exception as ex:\n",
    "\tprint (ex)\n",
    "else:\n",
    "\tprint(\"Tablas creadas con éxito\")\n",
    "finally:\t\n",
    "\tdb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lectura de las tablas y conversión del dataframe a una lista de diccionarios (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_sql(tabla):    \n",
    "    df= pd.read_sql(f\"select * from {tabla}\", engine) # lee x tabla (sql) y lo guarda en la variable df\n",
    "    dataset=df.to_dict(\"records\")  # transforma la variable df en un diccionario dividido en registros (dataset)\n",
    "    db.close()\n",
    "    return dataset\n",
    "    \n",
    "dataset_alumnos=leer_sql(\"alumnos\") # se generan los respectivos dataset\n",
    "dataset_profesores=leer_sql(\"profesores\")\n",
    "dataset_cursos_profesores=leer_sql(\"cursos_profesores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar las tablas recién generadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla eliminada exitosamente\n",
      "Conexión con MySQL cerrada\n",
      "Tabla eliminada exitosamente\n",
      "Conexión con MySQL cerrada\n",
      "Tabla eliminada exitosamente\n",
      "Conexión con MySQL cerrada\n"
     ]
    }
   ],
   "source": [
    "import MySQLdb\n",
    "def eliminar_tabla(sql): # función para eliminar una tabla determinada\n",
    "    try:\n",
    "        connection = MySQLdb.connect(host=\"127.0.0.1\", # conexión a la base de datos\n",
    "\t\t\t\t\t\tuser=\"root\",\n",
    "\t\t\t\t\t\tpasswd=\"Contadores2\",\n",
    "\t\t\t\t\t\tdb=\"pil_trabajo\",\n",
    "                        port=4000)\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(sql) # acá se inserta la sentencia sql (drop table x)\n",
    "        cursor.close()\n",
    "\n",
    "    except MySQLdb.Error as error:\n",
    "        print(\"Error al eliminar la tabla: {}\".format(error))\n",
    "    else:\n",
    "        print(\"Tabla eliminada exitosamente\")\n",
    "\n",
    "    finally:\n",
    "        if connection.ping:\n",
    "            connection.close()\n",
    "            print(\"Conexión con MySQL cerrada\")\n",
    "       \n",
    "eliminar_tabla(\"drop table alumnos\")\n",
    "eliminar_tabla(\"drop table profesores\")\n",
    "eliminar_tabla(\"drop table cursos_profesores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar el campo \"index\" de los dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_index(dataset): # función para eliminar el campo index del dataset\n",
    "    for x in dataset: #se recorre el dataset\n",
    "        del x[\"index\"]   # se elimina la clave \"index\" de cada registro\n",
    "                \n",
    "eliminar_index(dataset_alumnos)\n",
    "eliminar_index(dataset_profesores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraer valores únicos del campo \"gender\" de los dataset y traducirlos al español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traduccion_gender(dataset): # función para traducir los valores del campo \"gender\"\n",
    "    lista=[]\n",
    "    for x in dataset: # recorremos el dataset\n",
    "        if x[\"gender\"] not in lista: # si el el valor de gender no está en la lista, lo agregamos a dicha lista \n",
    "            lista.append(x[\"gender\"])    # de esta forma se obtienen los valores únicos que se utilizan a continuación\n",
    "\n",
    "    for x in dataset:\n",
    "        if x[\"gender\"]==\"Female\":\n",
    "            x[\"gender\"]=\"Femenino\"\n",
    "        elif x[\"gender\"]==\"Male\":\n",
    "            x[\"gender\"]=\"Masculino\"\n",
    "        elif x[\"gender\"]==\"Non-binary\":\n",
    "            x[\"gender\"]=\"No-binario\"\n",
    "        elif x[\"gender\"]==\"Polygender\":\n",
    "            x[\"gender\"]=\"Poligénero\"\n",
    "        elif x[\"gender\"]==\"Genderqueer\":\n",
    "            x[\"gender\"]=\"Género queer\"\n",
    "        elif x[\"gender\"]==\"Agender\":\n",
    "            x[\"gender\"]=\"Agénero\"\n",
    "        elif x[\"gender\"]==\"Bigender\":\n",
    "            x[\"gender\"]=\"Bigénero\"\n",
    "        elif x[\"gender\"]==\"Genderfluid\":\n",
    "            x[\"gender\"]=\"Género fluido\"   \n",
    "\n",
    "traduccion_gender(dataset_profesores)\n",
    "traduccion_gender(dataset_alumnos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertir los campos \"personal_id\" e \"instructor\" de str a número"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion_dni_numero(dataset): # función para convertir el formato de los campos personal_id e instructor, que serían el dni, de str a número\n",
    "    for x in dataset: # recorremos el dataset\n",
    "        if \"personal_id\" in x: # seleccionamos la clave personal_id                  \n",
    "            j=x[\"personal_id\"].replace('.','') # eliminamos los puntos mediante el método replace\n",
    "            x[\"personal_id\"]=j # asignamos un nuevo valor a la clave personal_id\n",
    "        if \"instructor\" in x: # lo mismo pero con el campo instructor\n",
    "            j=x[\"instructor\"].replace('.','')\n",
    "            x[\"instructor\"]=j    \n",
    "    \n",
    "conversion_dni_numero(dataset_cursos_profesores)\n",
    "conversion_dni_numero(dataset_alumnos)\n",
    "conversion_dni_numero(dataset_profesores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar registros repetidos en su totalidad o que tienen un campo repetido "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eliminar_registros_repetidos_entotalidad(dataset): # función para eliminar aquellos registros que se repiten en su totalidad\n",
    "    global nuevo_dataset  # hacemos la variable nuevo_dataset sea global para que exista por fuera de la función   \n",
    "    lista_unica = set()   # generamos un conjunto para almacenar los registros únicos pero en forma de tuplas\n",
    "    nuevo_dataset = []    # definimos una lista que va a almacenar los registros únicos en forma de diccionarios\n",
    "    repetidos = []  # lista que va a almacenar los registros eliminados\n",
    "\n",
    "    for j in dataset: # recorremos el dataset\n",
    "        t = tuple(j.items()) # convierte a la lista de tuplas de los pares clave-valor de cada registro en una tupla de tuplas\n",
    "        if t not in lista_unica: # si ese registro en forma de tupla de tuplas no está en el set lo agrega y a su \n",
    "                                 # vez agrega el registro en forma de diccionario a la lista nuevo_dataset\n",
    "            lista_unica.add(t)\n",
    "            nuevo_dataset.append(j)\n",
    "        else:                    # si no, se lo agrega al registro en forma de diccionario a la lista de repetidos\n",
    "            repetidos.append(j)      \n",
    "        \n",
    "\n",
    "def eliminar_registros_repetidos_uncampo(dataset,campo): # esta función sirve para eliminar aquellos registros que se repiten parcialmente, es decir en un campo determinado\n",
    "    valores_unicos = [] # crea una lista para almacenar todos los valores únicos del campo seleccionado\n",
    "    registros_repetidos = [] # crea una lista para almacenar todos los registros en los cuales en el campo seleccionado tengan un valor repetido\n",
    "    for x in dataset: # recorremos el dataset\n",
    "        for i,j in x.items():      # transformamos cada registro (diccionario) en una lista de tuplas \n",
    "            if i==campo:    # seleccionamos la clave que sea igual al campo elegido      \n",
    "                if j not in valores_unicos:      # si su valor no está en la lista, lo agregamos  \n",
    "                    valores_unicos.append(j)              \n",
    "                else:                         \n",
    "                    registros_repetidos.append(x)   # si no agregamos el diccionario a la lista de registros repetidos     \n",
    "    for x in dataset:   # se elimina del dataset todos aquellos registros que se encuentren en la lista de registros_repetidos\n",
    "        if x in registros_repetidos:\n",
    "            del dataset[dataset.index(x)]   \n",
    "    print(registros_repetidos) \n",
    "\n",
    "\n",
    "def buscar_registros_repetidos_uncampo(dataset,campo): # esta función es por si no se quiere eliminar aquellos registros con \n",
    "                                                       # campos repetidos, sino que solamente se los quiere encontrar para quizás modificar el \n",
    "                                                       # campo antes que borrar todo el registro\n",
    "    valores_unicos = [] \n",
    "    registros_repetidos = [] \n",
    "    for x in dataset:\n",
    "        for i,j in x.items():      \n",
    "            if i==campo:          \n",
    "                if j not in valores_unicos:        \n",
    "                    valores_unicos.append(j)              \n",
    "                else:                         \n",
    "                    registros_repetidos.append(x)     \n",
    "    print(registros_repetidos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear las tablas del archivo tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tablas import Base,engine,Profesores,Alumnos,Carreras,Profesores_Carreras,Alumnos_Carreras, Facultades,Ramas,Campus,Ubicacion,Provincias,Ciudades,Municipios,Paises,Genero\n",
    "\n",
    "db = engine.connect()\n",
    "\n",
    "Base.metadata.create_all(engine) # creación de las tablas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insertar datos en tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_3424\\1129181525.py:161: SAWarning: SELECT statement has a cartesian product between FROM element(s) \"ciudades\" and FROM element \"municipios\".  Apply join condition(s) between each element to resolve.\n",
      "  for row in municipio_alumno:\n",
      "C:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_3424\\1129181525.py:164: SAWarning: SELECT statement has a cartesian product between FROM element(s) \"provincias\" and FROM element \"ciudades\".  Apply join condition(s) between each element to resolve.\n",
      "  for row in provincia_alumno:\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "from Tablas import Base,engine,Profesores,Alumnos,Carreras,Profesores_Carreras,Alumnos_Carreras, Facultades,Ramas,Campus,Provincias,Ciudades,Municipios,Paises,Ubicacion,Genero\n",
    "\n",
    "Session = sessionmaker(bind = engine)\n",
    "session =  Session()\n",
    "\n",
    "# todos los dataset que van a ser rellenados con datos para después ser insertados en sus respectivas tablas\n",
    "\n",
    "dataset_facultades=[] \n",
    "dataset_campus=[] \n",
    "dataset_ramas=[] \n",
    "dataset_genero=[] \n",
    "dataset_ubicacion=[] \n",
    "dataset_profesores2=[] \n",
    "dataset_alumnos2=[] \n",
    "dataset_carreras=[] \n",
    "dataset_profesores_carreras=[] \n",
    "dataset_alumnos_carreras=[]\n",
    "dataset_ciudades=[] \n",
    "dataset_municipios=[] \n",
    "dataset_provincias=[] \n",
    "dataset_paises=[]\n",
    "\n",
    "def insertar_datos(dataset,tabla): # función para insertar los datos contenidos en cada dataset a su tabla correspondiente\n",
    "    lista = []\n",
    "\n",
    "    for x in dataset:\n",
    "        lista.append(tabla(**x))\n",
    "    session.add_all(lista)\n",
    "    session.commit()\n",
    "    session.close()\n",
    "\n",
    "# TABLAS: RAMAS, FACULTADES, CAMPUS Y GENERO\n",
    "\n",
    "def generar_dataset_RFCG(dataset_definitivo,dataset_original,campo,clave): #para generar los dataset de las tablas facultades, campus, ramas y genero\n",
    "    valores_unicos=[]      \n",
    "    for x in dataset_original: # recorremos el \"dataset_original\", que sería uno de los 3 dataset generados al principio a partir de los archivos csv\n",
    "        if x[campo] not in valores_unicos: # si el valor del campo seleccionado no está en la lista de valores únicos lo agregamos\n",
    "            valores_unicos.append(x[campo])\n",
    "            dataset_definitivo.append({clave:(x[campo])}) # y agregamos ese valor único junto con una clave determinada, todo en forma de un \n",
    "                                                        # diccionario, al dataset_definitivo, el que se va a usar \n",
    "                                                        # después para insertar los datos       \n",
    "\n",
    "generar_dataset_RFCG(dataset_facultades,dataset_cursos_profesores,\"institute\",\"nombre\") # generamos los dataset\n",
    "generar_dataset_RFCG(dataset_ramas,dataset_cursos_profesores,\"branch\",\"nombre\")\n",
    "generar_dataset_RFCG(dataset_campus,dataset_cursos_profesores,\"campus\",\"nombre\")\n",
    "generar_dataset_RFCG(dataset_genero,dataset_alumnos,\"gender\",\"tipo\")\n",
    "\n",
    "insertar_datos(dataset_ramas,Ramas) # insertamos los datos de los dataset en sus respectivas tablas\n",
    "insertar_datos(dataset_facultades,Facultades)\n",
    "insertar_datos(dataset_campus,Campus)\n",
    "insertar_datos(dataset_genero,Genero)\n",
    "\n",
    "# TABLAS: CIUDADES, MUNICIPIOS, PROVINCIAS Y PAÍSES\n",
    "\n",
    "def generar_dataset_CMPP(dataset_definitivo,campo): #para generar los dataset de las tablas ciudades, provincias, \n",
    "                                                    # municipios y países. El método es el mismo que en los anteriores con la diferencia\n",
    "                                                    # de que acá hay que recorrer dos dataset de los originales en vez de uno, \n",
    "                                                    # ya que los campos city, town, state y country\n",
    "                                                    # se encuentran en ambos dataset    \n",
    "    valores_unicos=[]\n",
    "    for x in dataset_alumnos:        \n",
    "            if x[campo] not in valores_unicos:\n",
    "                valores_unicos.append(x[campo])\n",
    "                dataset_definitivo.append({\"nombre\":(x[campo])})  \n",
    "    for x in dataset_profesores:        \n",
    "            if x[campo] not in valores_unicos:\n",
    "                valores_unicos.append(x[campo])\n",
    "                dataset_definitivo.append({\"nombre\":(x[campo])})       \n",
    "    \n",
    "generar_dataset_CMPP(dataset_ciudades,\"city\")\n",
    "generar_dataset_CMPP(dataset_municipios,\"town\")\n",
    "generar_dataset_CMPP(dataset_provincias,\"state\")\n",
    "generar_dataset_CMPP(dataset_paises,\"country\")\n",
    "\n",
    "insertar_datos(dataset_ciudades,Ciudades)\n",
    "insertar_datos(dataset_municipios,Municipios)\n",
    "insertar_datos(dataset_provincias,Provincias)\n",
    "insertar_datos(dataset_paises,Paises)\n",
    "\n",
    "# TABLA: UBICACION\n",
    "\n",
    "def generar_dataset_ubicacion(): # función para generar el dataset de la tabla ubicación, que tiene como clave foráneas a los id de las tablas ciudades, municipios y provincias\n",
    "    \n",
    "    for x in dataset_alumnos:\n",
    "        ciudades_id=session.query(Ciudades).filter(Ciudades.nombre==x[\"city\"]) # traigo todos los registros de la tabla ciudades cuyo campo nombre (el valor) sea igual al valor del campo \"city\" del dataset_alumnos\n",
    "        municipios_id=session.query(Municipios).filter(Municipios.nombre==x[\"town\"]) # lo mismo pero con el campo \"town\" (municipio)\n",
    "        provincias_id=session.query(Provincias).filter(Provincias.nombre==x[\"state\"]) # lo mismo pero con el campo \"state\" (provincia)\n",
    "        paises_id=session.query(Paises).filter(Paises.nombre==x[\"country\"]) # lo mismo pero con el campo \"country\" (provincia)\n",
    "        for row in ciudades_id: # recorro cada uno de los campos de los registros obtenidos anteriormente\n",
    "            dataset_ubicacion.append({\"ciudades_id\":row.id}) #agrego al dataset un diccionario cuyo valor se corresponde con el campo id de row\n",
    "        for row in municipios_id: \n",
    "            dataset_ubicacion[dataset_alumnos.index(x)].update({\"municipios_id\":row.id}) # la misma operación que la anterior para el campo municipios, variando que en vez de agregar un nuevo diccionario modifico el ya existento agregándole un nuevo par clave-valor\n",
    "        for row in provincias_id:\n",
    "            dataset_ubicacion[dataset_alumnos.index(x)].update({\"provincias_id\":row.id})\n",
    "        for row in paises_id:\n",
    "            dataset_ubicacion[dataset_alumnos.index(x)].update({\"paises_id\":row.id})\n",
    "            \n",
    "    for x in dataset_profesores: # repetimos todo lo hecho anteriormente para el dataset_profesores, ya que hay ciudades, municipios y provincias exclusivos de este dataset que no están en el de alumnos\n",
    "        ciudades_id=session.query(Ciudades).filter(Ciudades.nombre==x[\"city\"])\n",
    "        municipios_id=session.query(Municipios).filter(Municipios.nombre==x[\"town\"])\n",
    "        provincias_id=session.query(Provincias).filter(Provincias.nombre==x[\"state\"])\n",
    "        paises_id=session.query(Paises).filter(Paises.nombre==x[\"country\"])\n",
    "        for row in ciudades_id:\n",
    "            dataset_ubicacion.append({\"ciudades_id\":row.id})\n",
    "        for row in municipios_id:\n",
    "            dataset_ubicacion[dataset_profesores.index(x)+1000].update({\"municipios_id\":row.id}) # el +1000 en el index es para continuar desde el último registro del paso anterior, que inserta 1000 registros\n",
    "        for row in provincias_id:\n",
    "            dataset_ubicacion[dataset_profesores.index(x)+1000].update({\"provincias_id\":row.id})\n",
    "        for row in paises_id:\n",
    "            dataset_ubicacion[dataset_profesores.index(x)+1000].update({\"paises_id\":row.id})\n",
    "   \n",
    "generar_dataset_ubicacion()\n",
    "\n",
    "#Luego procedo a eliminar los registros repetidos del dataset_ubicacion con la función eliminar_registros_repetidos_entotalidad #\n",
    "\n",
    "eliminar_registros_repetidos_entotalidad(dataset_ubicacion)\n",
    "dataset_ubicacion=nuevo_dataset # Acá realizo una copia del nuevo_dataset generado con la función eliminar_registros, para sobrescribir los registros de dataset_ubicacion con el fin de hacer efectivos los cambios realizados con la función, de otra forma dataset_ubicacion permance sin alterar al ser una variable definida por fuera de la función\n",
    "\n",
    "insertar_datos(dataset_ubicacion,Ubicacion) \n",
    "\n",
    "# TABLA: CARRERAS\n",
    "\n",
    "def generar_dataset_carreras(): # función para generar el dataset de la tabla carreras que tiene como claves foráneas los id de ramas, facultades y campus          \n",
    "    for x in dataset_cursos_profesores:       \n",
    "        dataset_carreras.append({\"nombre\":x[\"program\"]})    \n",
    "        ramas_id=session.query(Ramas).filter(Ramas.nombre==x[\"branch\"]) # traigo todos los registros de la tabla ciudades cuyo campo nombre (el valor) sea igual al valor del campo \"branch\" del dataset_cursos_profesores\n",
    "        facultades_id=session.query(Facultades).filter(Facultades.nombre==x[\"institute\"]) # lo mismo pero con el campo \"institute\" (municipio)\n",
    "        campus_id=session.query(Campus).filter(Campus.nombre==x[\"campus\"]) # lo mismo pero con el campo \"campus\" (provincia)\n",
    "        for row in ramas_id: # recorro cada uno de los campos de los registros obtenidos anteriormente\n",
    "            dataset_carreras[dataset_cursos_profesores.index(x)].update({\"ramas_id\":row.id}) #agrego al dataset un diccionario cuyo valor se corresponde con el campo id de row\n",
    "        for row in facultades_id: \n",
    "            dataset_carreras[dataset_cursos_profesores.index(x)].update({\"facultades_id\":row.id}) # la misma operación que la anterior para el campo municipios, variando que en vez de agregar un nuevo diccionario modifico el ya existento agregándole un nuevo par clave-valor\n",
    "        for row in campus_id:\n",
    "            dataset_carreras[dataset_cursos_profesores.index(x)].update({\"campus_id\":row.id})\n",
    "\n",
    "generar_dataset_carreras()\n",
    "\n",
    "# Misma operación que en el dataset de ubicacion\n",
    "\n",
    "eliminar_registros_repetidos_entotalidad(dataset_carreras)\n",
    "dataset_carreras=nuevo_dataset\n",
    "\n",
    "insertar_datos(dataset_carreras,Carreras)\n",
    "\n",
    "# TABLAS: ALUMNOS Y PROFESORES\n",
    "\n",
    "def generar_dataset_AP(dataset_original,dataset_definitivo):  # Para generar los dataset de las tablas alumnos y profesores, que tienen como claves foráneas los id de las tablas ubicacion y carreras         \n",
    "    for x in dataset_original: # recorremos el dataset_original      \n",
    "        dataset_definitivo.append({\"dni\":x[\"personal_id\"]})  # agregamos al dataset_definitivo el campo dni con el valor del campo \"personal_id\" del dataset_original\n",
    "        dataset_definitivo[dataset_original.index(x)].update({\"nombre\":x[\"first_name\"]}) # agregamos el campo nombre\n",
    "        dataset_definitivo[dataset_original.index(x)].update({\"apellido\":x[\"last_name\"]}) # agregamos el campo apellido\n",
    "        dataset_definitivo[dataset_original.index(x)].update({\"email\":x[\"email\"]}) # agregamos el campo email\n",
    "        dataset_definitivo[dataset_original.index(x)].update({\"pais\":x[\"country\"]}) # agregamos el campo pais\n",
    "        dataset_definitivo[dataset_original.index(x)].update({\"fecha_nacimiento\":x[\"birthdate\"]}) # agregamos el campo fecha de nacimiento\n",
    "\n",
    "        ciudad_alumno=session.query(Ciudades).filter(Ciudades.nombre==x[\"city\"]) # para el campo ubicacion_id primero hay que obtener el registro de la tabla ciudades que se corresponde con el alumno de cada registro del dataset_original\n",
    "        for row in ciudad_alumno: # dentro de ese registro nos quedamos con el id\n",
    "            id_ciudad_alumno=row.id \n",
    "        municipio_alumno=session.query(Municipios).filter(Ciudades.nombre==x[\"town\"]) # lo mismo pero con municipios\n",
    "        for row in municipio_alumno:\n",
    "            id_municipio_alumno=row.id\n",
    "        provincia_alumno=session.query(Provincias).filter(Ciudades.nombre==x[\"state\"]) # lo mismo pero con provincias\n",
    "        for row in provincia_alumno:\n",
    "            id_provincia_alumno=row.id\n",
    "        ubicacion_id=session.query(Ubicacion).filter(Ubicacion.ciudades_id==id_ciudad_alumno and Ubicacion.municipios_id==id_municipio_alumno and Ubicacion.provincias_id==id_provincia_alumno) # traigo todos los registros de la tabla ubicacion en donde los id de ciudades, municipios y provincias coincidan con los id obtenidos anteriormente\n",
    "        for row in ubicacion_id: # recorro cada uno de los registros obtenidos anteriormente, que en realidad es un solo registro\n",
    "            dataset_definitivo[dataset_original.index(x)].update({\"ubicacion_id\":row.id}) #finalmente agrega el campo ubicacion_id con el id del registro obtenido \n",
    "        genero_id=session.query(Genero).filter(Genero.tipo==x[\"gender\"]) # agrego el último campo que es genero_id\n",
    "        for row in genero_id:\n",
    "            dataset_definitivo[dataset_original.index(x)].update({\"genero_id\":row.id})\n",
    "\n",
    "generar_dataset_AP(dataset_profesores,dataset_profesores2)\n",
    "generar_dataset_AP(dataset_alumnos,dataset_alumnos2)\n",
    "\n",
    "insertar_datos(dataset_alumnos2,Alumnos)\n",
    "insertar_datos(dataset_profesores2,Profesores)\n",
    "\n",
    "# TABLA: ALUMNOS_CARRERAS\n",
    "\n",
    "\n",
    "def generar_dataset_alumnos_carreras(): # función para generar el dataset correspondiente a la tabla carreras_alumnos\n",
    "    for x in dataset_alumnos:\n",
    "        alumno=session.query(Alumnos).filter(Alumnos.dni==x[\"personal_id\"]) # obtengo aquellos registros de la tabla alumnos en donde el valor del dni sea igual al del personal_id del registros seleccionado del dataset_original (dataset_alumnos)\n",
    "        carrera=session.query(Carreras).filter(Carreras.nombre==x[\"program\"]) # lo mismo pero con el nombre de la carrera\n",
    "        for row in alumno:            \n",
    "            alumno_id=row.id # obtenemos el id del registro seleccionado de alumnos\n",
    "        for row in carrera:\n",
    "            carrera_id=row.id # obtenemos el id del registro seleccionado de carreras\n",
    "        dataset_alumnos_carreras.append({\"alumno_id\":alumno_id,\"carrera_id\":carrera_id}) # agregamos el diccionario\n",
    "\n",
    "generar_dataset_alumnos_carreras()\n",
    "\n",
    "insertar_datos(dataset_alumnos_carreras,Alumnos_Carreras)\n",
    "\n",
    "# TABLA: PROFESORES_CARRERAS\n",
    "\n",
    "def generar_dataset_profesores_carreras(): # función para generar el dataset correspondiente a la tabla carreras_profesores, funciona igual que el de carreras_alumnos el método\n",
    "    for x in dataset_cursos_profesores:\n",
    "        profe=session.query(Profesores).filter(Profesores.dni==x[\"instructor\"])\n",
    "        carrera=session.query(Carreras).filter(Carreras.nombre==x[\"program\"])\n",
    "        for row in profe:            \n",
    "            profe_id=row.id\n",
    "        for row in carrera:\n",
    "            carrera_id=row.id\n",
    "        dataset_profesores_carreras.append({\"profesor_id\":profe_id,\"carrera_id\":carrera_id})\n",
    "        \n",
    "generar_dataset_profesores_carreras()\n",
    "\n",
    "insertar_datos(dataset_profesores_carreras,Profesores_Carreras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exportar tablas a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportar_csv(tabla,nombre_archivo):\n",
    "    df= pd.read_sql(f\"select * from {tabla}\", engine) # primero obtenemos el dataframe a partir de mysql\n",
    "    df.to_csv(f\"{nombre_archivo}.csv\",index=False, encoding=\"utf-8\") # después lo exportamos al formato csv, \n",
    "                                                    # eliminando el index que se agrega automáticamente \n",
    "                                                    # y codificándolo en utf-8 para su lectura en cualquier sistema \n",
    "                                                    # operativo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e913b7897ac30ba1cf0615ffc95e0dcac655edcf15ece0252fa093eec079069f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
